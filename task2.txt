2.1 Problems with only testing on a test set of 18 speeches is that the number of data for our test set is relatively small for a task like this, and so our outputted values can fluctuate largely, like accuracy value. Changes I would make to get a better evaluation would perhaps increase the number of data for test set, whether that's by getting more speeches elsewhere or by using Cross-Validation techniques to get better results. Potential drawbacks of this include time costs, since going through a larger test set or applying Cross-Validation methods can take significantly longer to do.
2.2 Yes, some predictions are more "certain" than others, and you can tell by comparing the LogProbabilities of each speech. The difference between the two probability values differ in some more than others, so those would be more "certain". If our model is good, then we can be more certain that the predicted label is correct if the difference in probabilities are greater. Some of the predicated classifications are wrong, however. We can see that these wrong classifications display relatively low confidence, in that their log probabilities barely differ. If the calculated probabilities barely differ in value, then we are less certain of what is the correct label and thus, some mistakes happen in these cases.
2.3 I extended my model to allow for both bigrams and trigrams, and I chose the bigram model to see if my classification accuracy improved, and it did. I think the increase in overall accuracy for my test data is because each bigram provides more information and context than a unigram would, so by shifting the majority of the n-gram model weights to bigram, our bigram model produced more accurate results. I tried weighing the weights differently for bigram and unigram and found that focusing heavily on bigram and decreasing the weight of unigram helped our accuracy. I also tried implementing a trigram model but found that for this dataset of speeches, the bigram model performed just fine and the trigram model was not necessary. Overall, using a linear interpolated language model improved our bayes classifier compared to a naive bayes classifier that only analyzed the word itself only.
